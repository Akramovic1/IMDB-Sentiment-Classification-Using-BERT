{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB Sentiment Classification Using BERT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyRQSx+C5GBHUrMW9RTvXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akramovic1/IMDB-Sentiment-Classification-Using-BERT/blob/main/IMDB_Sentiment_Classification_Using_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Movies reviews classification using BERT***\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3bQD0DhtGQjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement"
      ],
      "metadata": {
        "id": "XhdMOD56GU_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMDB is the most globally famous movie reviews website where you can publish a review for any film you watched. Classifying the positive reviews and the negative ones can be useful for several purposes such as giving an overall rating for the film or making statistical analysis about the preferences of people from different countries, age levels, etc... So IMDB dataset is released which composed of 50k reviews labeled as positive or negative to enable training movie reviews classifiers. Moreover, NLP tasks are currently solved based on pretrained language models such as BERT. These models provide a deep understanding of both semantic and contextual aspects of language words, sentences or even large paragraphs due to their training on huge corpus for very long time. In this notebook We will download the IMDB dataset from kaggle using this Link. Then, we will train BERT based classifier for movie reviews."
      ],
      "metadata": {
        "id": "lI2RtzKlGaPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "In this section we are going to get the problem data and install required libraries.\n",
        "First you have to **upload** the kaggle API token to the current running sesion then run the following cells."
      ],
      "metadata": {
        "id": "tI02XGSiGeiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "UrAWAunBGkuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data investigation"
      ],
      "metadata": {
        "id": "J0J2ypklGqJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dataframe and changing the lables to categorical\n",
        "df = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
        "df[\"sentiment\"] = df[\"sentiment\"].astype('category')\n",
        "df[\"labels\"] = df[\"sentiment\"].cat.codes\n",
        "df = df.drop(columns=['sentiment'])\n",
        "df.head()\n",
        "# Checking if undersampling or oversampling is needed\n",
        "print(df.columns)\n",
        "print(df.shape)\n",
        "sns.countplot(x='labels', data=df);"
      ],
      "metadata": {
        "id": "g3splVpNGuc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing"
      ],
      "metadata": {
        "id": "tXWFiKg5G-jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase all characters\n",
        "df[\"review\"] = df.review.apply(lambda x : str.lower(x))\n",
        "\n",
        "# Removing punctiations and <br />\n",
        "import re\n",
        "df[\"review\"] = df.review.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
        "df[\"review\"] = df.review.apply(lambda x : x.replace('br', ''))\n",
        "\n",
        "# Removing stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df[\"review\"] = df.review.apply(\n",
        "    lambda x : ' '.join(word for word in x.split() if word not in stop_words))\n",
        "df[\"review\"] = df.review.apply(lambda x : x.replace('   ', ' '))\n",
        "\n",
        "# Lemmatization\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
        "\n",
        "df[\"review\"] = df.review.apply(lambda x : lemmatize_text(x))\n",
        "df['review'] = df.review.apply(lambda x : ' '.join([str(elem) for elem in x]))\n",
        "df['review_length'] = df.review.apply(lambda x : len(x.split()))\n",
        "print(df.review_length.describe())\n",
        "df.drop(columns = 'review_length' , inplace = True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jGHskBnAG_bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Model"
      ],
      "metadata": {
        "id": "GW6JSxMpHOp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 256\n",
        "df['review'] = df.review.apply(lambda x : x[:max_words])\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.labels = list(df.labels)\n",
        "        self.texts = list(df.review.apply(lambda x :tokenizer(x, \n",
        "                               padding='max_length', max_length = 256, truncation=True,\n",
        "                                return_tensors=\"pt\")))\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ],
      "metadata": {
        "id": "fCcD1hXHHbh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratify and split\n",
        "train_list = []\n",
        "test_list = []\n",
        "val_list = []\n",
        "grouped_df = df.groupby('labels')\n",
        "\n",
        "for i, g in grouped_df:\n",
        "    train, test, val = np.split(g, [int(.7 * len(g)), int(.8 * len(g))])\n",
        "\n",
        "    train_list.append(train); test_list.append(test); val_list.append(val)\n",
        "\n",
        "df_train = pd.concat(train_list)\n",
        "df_val = pd.concat(test_list)\n",
        "df_test = pd.concat(val_list)\n",
        "\n",
        "df_train = df_train.sample(frac = 1)\n",
        "df_val = df_val.sample(frac = 1)\n",
        "df_test = df_test.sample(frac = 1)"
      ],
      "metadata": {
        "id": "Q8t-cS__Hpcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "l7fOD0t7Hx7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout_bert = nn.Dropout(0.5)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear1 = nn.Linear(768, 512)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
        "        self.linear2 = nn.Linear(512, 256)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(256)\n",
        "        self.linear3 = nn.Linear(256, 128)\n",
        "        self.linear4 = nn.Linear(128, 64)\n",
        "        self.linear_out = nn.Linear(64, 1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, x = self.bert(input_ids=input_id, attention_mask=mask,return_dict=False)\n",
        "        x = self.dropout_bert(x)\n",
        "        x = self.relu1(self.linear1(x))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu2(self.linear2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu3(self.linear3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu4(self.linear4(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.sigmoid(self.linear_out(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "zBu0piGbHyYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "_b5996raH9AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "epoch_list = list()\n",
        "train_loss = list()\n",
        "val_loss = list()\n",
        "train_accuracy = list()\n",
        "val_accuracy = list()\n",
        "\n",
        "def train(model, train_data, val_data, learning_rate, epochs):\n",
        "                                  \n",
        "    # Early stopping\n",
        "    the_last_loss = 100\n",
        "    patience = 2\n",
        "    trigger_times = 0\n",
        "\n",
        "    train, val = Dataset(train_data), Dataset(val_data)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=8,shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=8)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "                output = model(input_id, mask)\n",
        "\n",
        "                batch_loss = criterion(output.squeeze(1), train_label.float())\n",
        "                total_loss_train += batch_loss.item()\n",
        "\n",
        "                y_pred_tag = torch.round(output.squeeze(1))\n",
        "                acc = (y_pred_tag == train_label).sum()\n",
        "\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output.squeeze(1), val_label.float())\n",
        "                    total_loss_val += batch_loss.item()\n",
        "                    \n",
        "                    y_pred_tag = torch.round(output.squeeze(1))\n",
        "                    acc = (y_pred_tag == val_label).sum()\n",
        "                    total_acc_val += acc\n",
        "            \n",
        "            train_loss.append(total_loss_train / len(train_data))\n",
        "            train_accuracy.append(total_acc_train.item() / len(train_data))\n",
        "            val_loss.append(total_loss_val / len(val_data))\n",
        "            val_accuracy.append(total_acc_val.item() / len(val_data))\n",
        "            epoch_list.append(epoch_num+1)\n",
        "\n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
        "            \n",
        "            # Early stopping\n",
        "            the_current_loss = total_loss_val / len(val_data)\n",
        "            if the_current_loss > the_last_loss:\n",
        "                trigger_times += 1\n",
        " \n",
        "                if trigger_times >= patience:\n",
        "                    print('Early stopping!\\nStart to test process.')\n",
        "                    break\n",
        " \n",
        "            else:\n",
        "              trigger_times = 0\n",
        "\n",
        "            the_last_loss = the_current_loss\n",
        "            \n",
        "            \n",
        "                  \n",
        "EPOCHS = 20\n",
        "model = BertClassifier()\n",
        "LR = 1e-5\n",
        "\n",
        "train(model, df_train, df_val, LR, EPOCHS)"
      ],
      "metadata": {
        "id": "SrwWvdnxIAWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot"
      ],
      "metadata": {
        "id": "5rSqZXCPICXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = [18, 5])\n",
        "plt.suptitle(\"Loss and Accuracy in training\")\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epoch_list,train_loss);\n",
        "plt.plot(epoch_list,val_loss);\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Train\",\"Validation\"])\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epoch_list,train_accuracy);\n",
        "plt.plot(epoch_list,val_accuracy);\n",
        "plt.xlabel(\"Epochs\");\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"Train\",\"Validation\"]);"
      ],
      "metadata": {
        "id": "1KxbzH7yIEY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "pBm9lESBIHr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_data):\n",
        "\n",
        "    test = Dataset(test_data)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=32)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    final_confusion_matrix = np.zeros([2,2])\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "        for test_input, test_label in test_dataloader:\n",
        "\n",
        "              test_label = test_label.to(device)\n",
        "              mask = test_input['attention_mask'].to(device)\n",
        "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "              output = model(input_id, mask)\n",
        "              y_pred_tag = torch.round(output.squeeze(1))\n",
        "              acc = (y_pred_tag == test_label).sum()\n",
        "              total_acc_test += acc\n",
        "              final_confusion_matrix += confusion_matrix(test_label.cpu().data.numpy(), y_pred_tag.cpu().data.numpy())\n",
        "    \n",
        "    TP = final_confusion_matrix[0][0]\n",
        "    FP = final_confusion_matrix[0][1]\n",
        "    FN = final_confusion_matrix[1][0]\n",
        "    TN = final_confusion_matrix[1][1]\n",
        "    Percision = TP/(TP+FP)\n",
        "    Recall = TP/(TP+FN)\n",
        "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
        "    print(f'Percision : {Percision}')\n",
        "    print(f'Sensitivity (Recall) : {Recall}')\n",
        "    print(f'Specifity : {TN/(TN+FP)}')\n",
        "    print(f'F1 Score = {((2*Percision*Recall)/(Percision+Recall))}')\n",
        "    print('Confusion Matrix :')\n",
        "    print(final_confusion_matrix)\n",
        "    \n",
        "evaluate(model, df_test)"
      ],
      "metadata": {
        "id": "vLTGlb09IP-V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}